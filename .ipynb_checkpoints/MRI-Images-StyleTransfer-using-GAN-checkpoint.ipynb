{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f9a5e1a",
   "metadata": {},
   "source": [
    "# MRI Images StyleTransfer using GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf606574",
   "metadata": {},
   "source": [
    "### Problem Statement :- \n",
    "\n",
    "### To translate the style of one MRI image into another. This will help in gaining a better understanding and analysis of the scanned image. By using GANs, we will create T2 weighted images from T1 weighted MRI images and vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2bce5e",
   "metadata": {},
   "source": [
    "MRI T1 and T2 Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f3b36b",
   "metadata": {},
   "source": [
    "<table border=\"1\" cellpadding=\"1\" cellspacing=\"1\" align=\"left\">\n",
    "\t<tbody>\n",
    "\t\t<tr>\n",
    "\t\t\t<th>Type</th>\n",
    "\t\t\t<th>T1 Highlight Style</th>\n",
    "\t\t\t<th>T2 Highlight Style</th>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td><b>Water</b></td>\n",
    "\t\t\t<td>Dark</td>\n",
    "\t\t\t<td>Very Bright</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td><b>Fat</b></td>\n",
    "\t\t\t<td>Very Bright</td>\n",
    "\t\t\t<td>Dark</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td><b>Bone</b></td>\n",
    "\t\t\t<td>Dark</td>\n",
    "\t\t\t<td>Dark</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td><b>Muscle</b></td>\n",
    "\t\t\t<td>Intermediate</td>\n",
    "\t\t\t<td>Dark</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td><b>Tumours</b></td>\n",
    "\t\t\t<td>Intermediate</td>\n",
    "\t\t\t<td>Bright</td>\n",
    "\t\t</tr>\n",
    "\t</tbody>\n",
    "</table>\n",
    "\n",
    "<p>&nbsp;</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e7614",
   "metadata": {},
   "source": [
    "<p align=\"left\">Example of MRI T1 to T2 Transition Image </p>\n",
    "<img src=\"T1_to_T2_ImageTransition.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6836b325",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "The Sequence of the Project\n",
    "\n",
    "1. Importing Libraries\n",
    "2. Data Loading and Visualization\n",
    "3. Data Preprocessing\n",
    "4. Model Building\n",
    "5. Model Training\n",
    "6. Generating a GIF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae1e610",
   "metadata": {},
   "source": [
    "### 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c049fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import imageio, os, glob\n",
    "import cv2\n",
    "import random as rn\n",
    "\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f18461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed values \n",
    "np.random.seed(30)\n",
    "rn.seed(30)\n",
    "tf.random.set_seed(30)\n",
    "seed = 30\n",
    "\n",
    "# set batch size for image processing\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f55a039",
   "metadata": {},
   "source": [
    "### 2. Data Loading and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c85ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Images path and getting image names\n",
    "T1_image_path = 'D:\\\\UpGrad_Notes\\\\capstone\\\\StyleTransterUsingGAN\\\\MRI+T1_T2+Dataset\\\\Tr1\\\\TrainT1\\\\'\n",
    "T2_image_path = 'D:\\\\UpGrad_Notes\\\\capstone\\\\StyleTransterUsingGAN\\\\MRI+T1_T2+Dataset\\\\Tr2\\\\TrainT2\\\\'\n",
    "\n",
    "T1_image_names = os.listdir(T1_image_path)\n",
    "T2_image_names = os.listdir(T2_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efaefdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing a sample image\n",
    "img_T1 = cv2.imread(T1_image_path+T1_image_names[0])\n",
    "img_T2 = cv2.imread(T2_image_path+T2_image_names[0])\n",
    "\n",
    "plt.figure(figsize=(4,6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('T1 Image', fontsize=16, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.imshow(img_T1, cmap='gray')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('T2 Image', fontsize=16, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.imshow(img_T2, cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9dd612",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013aed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Normalizing, resizing and loading the images\n",
    "\n",
    "T1_data = np.zeros((len(T1_image_names), 128, 128, 3))\n",
    "T2_data = np.zeros((len(T2_image_names), 128, 128, 3))\n",
    "\n",
    "for index, img in enumerate(T1_image_names):\n",
    "    imgT1 = cv2.imread(T1_image_path+img)\n",
    "    imgT1 = cv2.cvtColor(imgT1, cv2.COLOR_BGR2RGB)\n",
    "    imgT1 = (imgT1/127.5)-1.0                           # Normalize the image to [-1.0, 1.0]\n",
    "    T1_data[index, :, :, :] = resize(imgT1, (128, 128)) # resizing the image\n",
    "    \n",
    "for index, img in enumerate(T2_image_names):\n",
    "    imgT2 = cv2.imread(T2_image_path+img)\n",
    "    imgT2 = cv2.cvtColor(imgT2, cv2.COLOR_BGR2RGB)\n",
    "    imgT2 = (imgT2/127.5)-1.0                            # Normalize the image to [-1.0, 1.0]\n",
    "    T2_data[index, :, :, :] = resize(imgT2, (128, 128))  # resizing the image\n",
    "\n",
    "# changing dtype to float32\n",
    "T1_data = T1_data.astype('float32')\n",
    "T2_data = T2_data.astype('float32')\n",
    "\n",
    "print(f'T1 shape: {T1_data.shape}')\n",
    "print(f'T2 shape: {T2_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcbd3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking normalized sample\n",
    "T1_data[1,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca50f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating batch and shuffling data\n",
    "\n",
    "T1_data_batch = tf.data.Dataset.from_tensor_slices(T1_data).shuffle(T1_data.shape[0], seed=seed).batch(BATCH_SIZE)\n",
    "T2_data_batch = tf.data.Dataset.from_tensor_slices(T2_data).shuffle(T2_data.shape[0], seed=seed).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ca7e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing a sample\n",
    "\n",
    "sample_T1_data = next(iter(T1_data_batch))\n",
    "sample_T2_data = next(iter(T2_data_batch))\n",
    "\n",
    "plt.figure(figsize=(4, 6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(sample_T1_data[0, :, :, 0], cmap='gray')\n",
    "plt.title('T1_data')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(sample_T2_data[0, :, :, 0], cmap='gray')\n",
    "plt.title('T2_data')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1402a728",
   "metadata": {},
   "source": [
    "### 4. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac67ae6",
   "metadata": {},
   "source": [
    "#### Building generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a075035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance Normalization\n",
    "class InstanceNormalization(tf.keras.layers.Layer):\n",
    "    # Initialization of Objects\n",
    "    def __init__(self, epsilon=1e-5):\n",
    "        # calling parent's init\n",
    "        super(InstanceNormalization, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.scale = self.add_weight(\n",
    "            name='scale',\n",
    "            shape=input_shape[-1:],\n",
    "            initializer=tf.random_normal_initializer(1., 0.02),\n",
    "            trainable=True)\n",
    "        self.offset = self.add_weight(\n",
    "            name='offset',\n",
    "            shape=input_shape[-1:],\n",
    "            initializer='zeros',\n",
    "            trainable=True)\n",
    "    \n",
    "    def call(self, x):\n",
    "        # Compute Mean and Variance, Axes=[1,2] ensures Instance Normalization\n",
    "        mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n",
    "        inv = tf.math.rsqrt(variance + self.epsilon)\n",
    "        normalized = (x - mean) * inv\n",
    "        return self.scale * normalized + self.offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6641874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling is performed using the Convolution, leading to reduce in dimensions.\n",
    "def Downsampling(filters, size, apply_norm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    result = tf.keras.Sequential()\n",
    "    # Add Conv2d layer\n",
    "    result.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                                      kernel_initializer=initializer, use_bias=False))\n",
    "    # Add Normalization layer\n",
    "    if apply_norm:\n",
    "        result.add(InstanceNormalization())\n",
    "    # Add Leaky Relu Activation\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e2782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsampling is a result of Transposed Convolution, where dimension of image are increased.\n",
    "def Upsampling(filters, size, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    result = tf.keras.Sequential()\n",
    "    # Add Transposed Conv2d layer\n",
    "    result.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same',\n",
    "                                               kernel_initializer=initializer, use_bias=False))\n",
    "    # Add Normalization Layer\n",
    "    result.add(InstanceNormalization())\n",
    "    # Conditionally add Dropout layer\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "    # Add Relu Activation Layer\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f5cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unet Generator is a combination of Convolution + Transposed Convolution Layers\n",
    "def Unet_Generator():\n",
    "    down_stack = [\n",
    "        Downsampling(64, 4, False), # (bs, 64, 64, 64)\n",
    "        Downsampling(128, 4), # (bs, 32, 32, 128)\n",
    "        Downsampling(128, 4), # (bs, 16, 16, 128)\n",
    "        Downsampling(128, 4), # (bs, 8, 8, 128)\n",
    "        Downsampling(128, 4), # (bs, 4, 4, 128)\n",
    "        Downsampling(128, 4), # (bs, 2, 2, 128)\n",
    "        Downsampling(128, 4) # (bs, 1, 1, 128)\n",
    "    ]\n",
    "    up_stack = [\n",
    "        Upsampling(128, 4, True), # (bs, 2, 2, 256)\n",
    "        Upsampling(128, 4, True), # (bs, 4, 4, 256)\n",
    "        Upsampling(128, 4, True), # (bs, 8, 8, 256)\n",
    "        Upsampling(128, 4, True), # (bs, 16, 16, 256)\n",
    "        Upsampling(128, 4), # (bs, 32, 32, 256)\n",
    "        Upsampling(64, 4) # (bs, 64, 64, 64)\n",
    "    ]\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = tf.keras.layers.Conv2DTranspose(3, 4, strides=2, padding='same', kernel_initializer=initializer,\n",
    "                                           activation='tanh') # (bs, 128, 128, 3)\n",
    "    concat = tf.keras.layers.Concatenate()\n",
    "    inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n",
    "    x = inputs\n",
    "    # Downsampling through the model\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "    \n",
    "    skips = reversed(skips[:-1])\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = concat([x, skip])\n",
    "    x = last(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7f80ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing generator T1_T2/ T2_T1\n",
    "generator_T1_T2 = Unet_Generator()\n",
    "generator_T2_T1 = Unet_Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2358204",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_T2_T1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdcd73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminators only contain Convolutional Layers and no Transposed Convolution is not used \n",
    "def Discriminator():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    # add input layer of size (128, 128, 3)\n",
    "    inp = tf.keras.layers.Input(shape=[128, 128, 3], name='input_image')\n",
    "    x = inp\n",
    "    \n",
    "    # add downsampling step here\n",
    "    down1 = Downsampling(64, 4, False)(x) # (bs, 64, 64, 64)\n",
    "    down2 = Downsampling(128, 4)(down1) # (bs, 32, 32, 128)\n",
    "    down3 = Downsampling(128, 4)(down2)\n",
    "    down4 = Downsampling(128, 4)(down3)\n",
    "    # add a padding layer here\n",
    "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down4) # (bs, 10, 10, 128)\n",
    "    \n",
    "    # implement a concrete downsampling layer here\n",
    "    conv = tf.keras.layers.Conv2D(256, 4, strides=1, kernel_initializer=initializer,\n",
    "                                  use_bias=False)(zero_pad1) # (bs, 7, 7, 256)\n",
    "    norm1 = InstanceNormalization()(conv)\n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(norm1)\n",
    "    \n",
    "    # apply zero padding layer\n",
    "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 9, 9, 256)\n",
    "    \n",
    "    # add a last pure 2D Convolution layer\n",
    "    last = tf.keras.layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(zero_pad2) # (bs, 6, 6, 1)\n",
    "    return tf.keras.Model(inputs=inp, outputs=last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f5fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_x = Discriminator()\n",
    "discriminator_y = Discriminator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a99ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_x.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# untrained generator images check\n",
    "\n",
    "T1T2_data = generator_T1_T2(sample_T1_data)\n",
    "T2T1_data = generator_T2_T1(sample_T2_data)\n",
    "plt.figure(figsize=(4, 4))\n",
    "\n",
    "imgs = [sample_T1_data, T1T2_data, sample_T2_data, T2T1_data]\n",
    "title = ['Sample_T1_data', 'T1T2_data', 'Sample_T2_data', 'T2T1_data']\n",
    "\n",
    "plt.figure(figsize=(4,6))\n",
    "for i in range(len(imgs)):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.title(title[i])\n",
    "    plt.imshow(imgs[i][0].numpy()[:, :, 0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657a2aba",
   "metadata": {},
   "source": [
    "#### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adce83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting loss functions\n",
    "\n",
    "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def Discriminator_Loss(real, generated):\n",
    "    real_loss = loss_obj(tf.ones_like(real), real)\n",
    "    generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    return total_disc_loss * 0.5 # mean of losses\n",
    "\n",
    "def Generator_Loss(generated):\n",
    "    return loss_obj(tf.ones_like(generated), generated)\n",
    "\n",
    "def Calc_cycle_Loss(real_image, cycled_image):\n",
    "    loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "    return 10.0 * loss1\n",
    "\n",
    "def Identity_Loss(real_image, same_image):\n",
    "    loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "    return 0.5*loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7a6025",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2270b1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "\n",
    "generator_T1_T2_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "generator_T2_T1_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "discriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bfeaf5",
   "metadata": {},
   "source": [
    "#### Checkpoint Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f61f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"D:\\\\UpGrad_Notes\\\\capstone\\\\Git\\\\jagdishj\\\\checkpoint\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator_T1_T2=generator_T1_T2,\n",
    "                           generator_T2_T1=generator_T2_T1,\n",
    "                           discriminator_x=discriminator_x,\n",
    "                           discriminator_y=discriminator_y,\n",
    "                           generator_T1_T2_optimizer=generator_T1_T2_optimizer,\n",
    "                           generator_T2_T1_optimizer=generator_T2_T1_optimizer,\n",
    "                           discriminator_x_optimizer=discriminator_x_optimizer,\n",
    "                           discriminator_y_optimizer=discriminator_y_optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57937ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show Images output by Generators while Training\n",
    "def generate_images(model1, test_input1, model2, test_input2):\n",
    "    prediction1 = model1(test_input1)\n",
    "    prediction2 = model2(test_input2)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    display_list = [test_input1[0], prediction1[0], test_input2[0], prediction2[0]]\n",
    "    title = ['Input Image', 'Predicted Image', 'Input Image', 'Predicted Image']\n",
    "    for i in range(4):\n",
    "        plt.subplot(1, 4, i+1)\n",
    "        plt.title(title[i],fontsize =8)\n",
    "        plt.imshow(display_list[i].numpy()[:, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5856e6dc",
   "metadata": {},
   "source": [
    "#### Training flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61263596",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_x, real_y):\n",
    "    # persistent is set to True because the tape is used more than\n",
    "    # once to calculate the gradients.\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Generator G translates X -> Y\n",
    "        # Generator F translates Y -> X\n",
    "        fake_y = generator_T1_T2(real_x, training=True)\n",
    "        fake_x = generator_T2_T1(real_y, training=True)\n",
    "      \n",
    "        cycled_y = generator_T1_T2(fake_x, training=True)\n",
    "        cycled_x = generator_T2_T1(fake_y, training=True)\n",
    "        \n",
    "        # same_x and same_y are used for identity loss.\n",
    "        same_x = generator_T2_T1(real_x, training=True)\n",
    "        same_y = generator_T1_T2(real_y, training=True)\n",
    "        \n",
    "        disc_real_x = discriminator_x(real_x, training=True)\n",
    "        disc_real_y = discriminator_y(real_y, training=True)\n",
    "        \n",
    "        disc_fake_x = discriminator_x(fake_x, training=True)\n",
    "        disc_fake_y = discriminator_y(fake_y, training=True)\n",
    "        \n",
    "        # calculate the loss\n",
    "        gen_T1_T2_loss = Generator_Loss(disc_fake_y)\n",
    "        gen_T2_T1_loss = Generator_Loss(disc_fake_x)\n",
    "        \n",
    "        total_cycle_loss = Calc_cycle_Loss(real_x, cycled_x) + Calc_cycle_Loss(real_y, cycled_y)\n",
    "        \n",
    "        # Total generator loss = BCE loss + cycle loss + identity loss\n",
    "        total_gen_T1_T2_loss = gen_T1_T2_loss + total_cycle_loss + Identity_Loss(real_y, same_y)\n",
    "        total_gen_T2_T1_loss = gen_T2_T1_loss + total_cycle_loss + Identity_Loss(real_x, same_x)\n",
    "        \n",
    "        # Discriminator's loss\n",
    "        disc_x_loss = Discriminator_Loss(disc_real_x, disc_fake_x)\n",
    "        disc_y_loss = Discriminator_Loss(disc_real_y, disc_fake_y)\n",
    "        \n",
    "    # Calculate the gradients for generator and discriminator\n",
    "    generator_T1_T2_gradients = tape.gradient(total_gen_T1_T2_loss, generator_T1_T2.trainable_variables)\n",
    "    generator_T2_T1_gradients = tape.gradient(total_gen_T2_T1_loss, generator_T2_T1.trainable_variables)\n",
    "    \n",
    "    discriminator_x_gradients = tape.gradient(disc_x_loss, discriminator_x.trainable_variables)\n",
    "    discriminator_y_gradients = tape.gradient(disc_y_loss, discriminator_y.trainable_variables)\n",
    "    \n",
    "    # Apply the gradients to the optimizer\n",
    "    generator_T1_T2_optimizer.apply_gradients(zip(generator_T1_T2_gradients, generator_T1_T2.trainable_variables))\n",
    "    generator_T2_T1_optimizer.apply_gradients(zip(generator_T2_T1_gradients, generator_T2_T1.trainable_variables))\n",
    "    \n",
    "    discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients, discriminator_x.trainable_variables))\n",
    "    discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients, discriminator_y.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9e846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    for image_x, image_y in tf.data.Dataset.zip((T1_data_batch, T2_data_batch)):\n",
    "        train_step(image_x, image_y)\n",
    "    generate_images(generator_T1_T2, sample_T1_data, generator_T2_T1, sample_T2_data)\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print('Saving checkpoint for epoch', epoch, 'at', ckpt_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de14845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating animation for displaying per epoch changes\n",
    "anim_file = 'cyclegan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob('image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  for filename in filenames:\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb1f2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying gif animation using tensorflow docs\n",
    "!pip install -q git+https://github.com/tensorflow/docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5679ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying gif animation using tensorflow docs\n",
    "import tensorflow_docs.vis.embed as embed\n",
    "embed.embed_file(anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ce60e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
